import torch
import torch.nn as nn
import math
import einops
import torch.utils.checkpoint
import torch.nn.functional as F
import numpy as np
import warnings

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
    

def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
    ATTENTION_MODE = 'flash'
else:
    try:
        import xformers
        import xformers.ops
        ATTENTION_MODE = 'xformers'
    except:
        ATTENTION_MODE = 'math'
print(f'attention mode is {ATTENTION_MODE}')


def timestep_embedding(timesteps, dim, max_period=10000):
    """
    Create sinusoidal timestep embeddings.

    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding


def patchify(imgs, patch_size):
    x = einops.rearrange(imgs, 'B C (h p1) (w p2) -> B (h w) (p1 p2 C)', p1=patch_size, p2=patch_size)
    return x


def unpatchify(x, channels=3):
    patch_size = int((x.shape[2] // channels) ** 0.5)
    h = w = int(x.shape[1] ** .5)
    assert h * w == x. shape[1] and patch_size ** 2 * channels == x.shape[2]
    x = einops.rearrange(x, 'B (h w) (p1 p2 C) -> B C (h p1) (w p2)', h=h, p1=patch_size, p2=patch_size)
    return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, L, C = x.shape

        qkv = self.qkv(x)
        if ATTENTION_MODE == 'flash':
            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads).float()
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            x = torch.nn.functional.scaled_dot_product_attention(q, k, v)
            x = einops.rearrange(x, 'B H L D -> B L (H D)')
        elif ATTENTION_MODE == 'xformers':
            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B L H D', K=3, H=self.num_heads)
            q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D
            x = xformers.ops.memory_efficient_attention(q, k, v)
            x = einops.rearrange(x, 'B L H D -> B L (H D)', H=self.num_heads)
        elif ATTENTION_MODE == 'math':
            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads)
            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = (attn @ v).transpose(1, 2).reshape(B, L, C)
        else:
            raise NotImplemented

        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip=False, use_checkpoint=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale)
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)
        self.skip_linear = nn.Linear(2 * dim, dim) if skip else None
        self.use_checkpoint = use_checkpoint

    def forward(self, x, skip=None):
        if self.use_checkpoint:
            return torch.utils.checkpoint.checkpoint(self._forward, x, skip)
        else:
            return self._forward(x, skip)

    def _forward(self, x, skip=None):
        if self.skip_linear is not None:
            x = self.skip_linear(torch.cat([x, skip], dim=-1))
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x



class UViTLin(nn.Module):
    def __init__(self, embed_dim=128, feat_dim=384, num_clusters=10,mlp_time_embed=True,self_condition=True,sigma=True):
        super().__init__()
        
        
        self.embed_dim = embed_dim
        self.feat_dim = feat_dim
        self.self_condition = self_condition
        self.num_clusters = num_clusters
        self.sigma = sigma
        self.embed_dim_start =(embed_dim*2) if self.self_condition else embed_dim
        self.in_channels = self.embed_dim_start + self.feat_dim
        self.out_channels = (embed_dim*2) if self.sigma else embed_dim
        
        self.dropout1 = nn.Dropout(0.15)
        self.dropout2 = nn.Dropout(0.0)
        self.dropout3 = nn.Dropout(0.0)
        self.time_embed = nn.Sequential(
            nn.Linear(self.embedding_dim_start, 4 * self.embedding_dim_start),
            nn.SiLU(),
            nn.Linear(4 * self.embedding_dim_start, self.embedding_dim_start),
        ) if mlp_time_embed else nn.Identity()

        self.cluster_tokens = nn.Parameter(torch.randn(1, num_clusters*0+1, self.embed_dim))
        self.clusters_centers = nn.Parameter(torch.randn(1,num_clusters,self.embed_dim)).requires_grad_(True)

        self.data_input_proj = nn.Sequential(
            nn.Linear(self.feat_dim, self.feat_dim, bias=True),
            nn.SiLU(),
            nn.Linear(self.feat_dim, self.feat_dim, bias=True),
            nn.LayerNorm(self.feat_dim, elementwise_affine=False, eps=1e-6),
            nn.Linear(self.feat_dim, self.feat_dim, bias=True),
        )

        self.cluster_input_proj = nn.Sequential(
            nn.Linear(self.embedding_dim_start*2, self.embedding_dim_start*2, bias=True),
            nn.SiLU(),
            nn.Linear(self.embedding_dim_start*2,  self.embedding_dim_start*2, bias=True),
            nn.LayerNorm(self.embedding_dim_start, elementwise_affine=True, eps=1e-6),
            nn.Linear(self.embedding_dim_start*2,  self.embedding_dim_start*2, bias=True),
        )
        
        self.final_layer = nn.Linear(self.embedding_dim , self.embedding_dim)
        self.last_layer =  nn.Linear(self.embedding_dim, num_clusters)


        self.network = nn.Sequential(
            nn.Linear(self.embed_dim, self.embed_dim),
            nn.SiLU(),
            nn.Linear(self.embed_dim, self.embed_dim),
            nn.SiLU(),##### NEW!
            nn.Linear(self.embed_dim, self.embedding_dim),
            nn.SiLU(),
            nn.Linear(self.embed_dim*2, self.embedding_dim),
            nn.SiLU(),
            nn.Linear(self.embedding_dim, self.embedding_dim),
        )

  
    def return_embedding(self,x):
        clusters_centers = F.normalize(self.clusters_centers,dim=-1)
        x_expanded = x.unsqueeze(-1)
        weighted = x_expanded * clusters_centers
        weighted_sum = torch.sum(weighted, dim=2)
        return weighted_sum*np.sqrt(self.embedding_dim)
    

    def return_last_layer(self,x):
        x = self.dropout3(x)
        return self.last_layer(x)

    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed'}

    def forward(self, x, timesteps,x_self_cond=False,return_feat=None,):
        cluster_assignments = x[0]
        if self.self_condition:
            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(cluster_assignments))
            cluster_assignments = torch.cat((x_self_cond, cluster_assignments), dim = -1)
        data_features = x[1]
        data_features = nn.functional.normalize(data_features, dim=-1, p=2)
        time_token = self.time_embed(timestep_embedding(timesteps, self.embedding_dim_start))
        time_token = time_token.unsqueeze(dim=1)
        data_features = self.dropout1(self.data_input_proj(data_features))
        cluster_assignments = self.dropout2(self.cluster_input_proj(torch.cat((cluster_assignments,time_token.repeat(1,cluster_assignments.shape[1],1)),-1)))

        combined_features = torch.cat((data_features,cluster_assignments),-1)
   
        x = combined_features
        x = self.network(x)
        #x = self.norm(x)
        final_output = self.final_layer(x)
        return final_output

    def get_clusters_centers_tensor(self):
    # Return the tensor data of clusters_centers, not as a Parameter
        return self.clusters_centers.data
    


class UViTLinCifar3(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=384, depth=4, num_heads=4, mlp_ratio=4.,
                 qkv_bias=False, qk_scale=None, norm_layer=nn.LayerNorm, mlp_time_embed=False, num_classes=-1,
                 use_checkpoint=False, conv=True, skip=True,num_clusters=10,multiple_k = 0, concat = 1):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_classes = num_classes
        self.in_chans = in_chans
        self.self_condition = True
        self.embedding_dim_start = 256 if self.self_condition else 128
        self.embedding_dim = 128

        self.feat_size = 384
        self.feat_size_start = 384
        self.num_clusters = num_clusters
        self.dropout1 = nn.Dropout(0.0)
        self.dropout2 = nn.Dropout(0.0)
        self.dropout3 = nn.Dropout(0.15)
        self.channels = 1
        self.embed_dim = self.embedding_dim_start+self.feat_size
        embed_dim = self.embed_dim
        self.time_embed = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.SiLU(),
            nn.Linear(4 * embed_dim, embed_dim),
        ) if mlp_time_embed else nn.Identity()

        self.in_blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                norm_layer=norm_layer, use_checkpoint=use_checkpoint)
            for _ in range((depth//2)*0+1)])


        self.norm = norm_layer(embed_dim)
        self.patch_dim = patch_size ** 2 * in_chans
        self.decoder_pred = nn.Linear(embed_dim, self.patch_dim, bias=True)

        self.cluster_tokens = nn.Parameter(torch.randn(1, num_clusters*0+1, self.embed_dim))

        self.clusters_centers = nn.Parameter(torch.randn(1,num_clusters,self.embedding_dim)).requires_grad_(True)
        self.input_data_layer = nn.Linear(num_clusters,  self.embed_dim)
        self.data_input_proj = nn.Sequential(
            nn.Linear(self.feat_size_start, self.feat_size_start, bias=True),
            nn.SiLU(),
            nn.Linear(self.feat_size_start, self.feat_size_start, bias=True),
            nn.LayerNorm(self.feat_size_start, elementwise_affine=False, eps=1e-6),
            nn.Linear(self.feat_size_start, self.feat_size, bias=True),
        )

        self.cluster_input_proj = nn.Sequential(
            nn.Linear(self.embedding_dim_start, self.embedding_dim_start, bias=True),
            nn.SiLU(),
            nn.Linear(self.embedding_dim_start,  self.embedding_dim_start, bias=True),
            nn.LayerNorm(self.embedding_dim_start, elementwise_affine=True, eps=1e-6),
            nn.Linear(self.embedding_dim_start,  self.embedding_dim_start, bias=True),
        )
        
        self.final_layer = nn.Linear(self.embed_dim , self.embedding_dim)
        self.last_layer =  nn.Linear(self.embedding_dim, num_clusters)

        self.combination_net = nn.Sequential(
            nn.Linear(3 * self.embedding_dim, 2*self.embedding_dim),
            nn.SiLU(), 
            nn.Linear(2 * self.embedding_dim, 1*self.embedding_dim),
        )

    def return_embedding(self,x):
        clusters_centers = F.normalize(self.clusters_centers,dim=-1)
        # temp = torch.matmul(x,clusters_centers.repeat(x.shape[0],1,1))
        x_expanded = x.unsqueeze(-1)

        # Broadcast and multiply
        # Now clusters_centers_normalized will automatically broadcast to match x_expanded's shape during multiplication
        weighted = x_expanded * clusters_centers

        # Sum across the clusters dimension (dim=2) to get the weighted sum
        # Result will have shape [batch, num_points, embedding_dim]
        weighted_sum = torch.sum(weighted, dim=2)
        return weighted_sum


    
    def return_weighted_embedding(self,x):
        #return self.layer_norm(x)
        return F.normalize(x,dim=-1)*np.sqrt(self.embedding_dim)
   
    
    def return_last_layer(self,x):
        x = self.dropout3(x)
        return self.last_layer(x)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x, timesteps,x_self_cond=False,return_feat=None,):
        cluster_assignments = x[0]
        if self.self_condition:
            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(cluster_assignments))
            cluster_assignments = torch.cat((x_self_cond, cluster_assignments), dim = -1)
        data_features = x[1]
        data_features = nn.functional.normalize(data_features, dim=-1, p=2)
        batch_size, num_points, _ = data_features.size()
        #.permute(0,2,1)
        # Project and combine data with cluster assignments
        data_features = self.dropout1(self.data_input_proj(data_features))
        cluster_assignments = self.dropout2(self.cluster_input_proj(cluster_assignments))
        #combined_features = self.cross_attention(data_features, cluster_assignments)
        combined_features = torch.cat((data_features,cluster_assignments),-1)
        #combined_features = data_features+cluster_assignments
        time_token = self.time_embed(timestep_embedding(timesteps, self.embed_dim))
        time_token = time_token.unsqueeze(dim=1)
        cluster_tokens = self.cluster_tokens.repeat(batch_size, 1, 1)
        x = torch.cat((cluster_tokens, combined_features), dim=1)
        x = torch.cat((time_token, x), dim=1)

        for blk in self.in_blocks:
            x = blk(x)

        x = self.norm(x)
        final_output = self.final_layer(x[:, self.channels+1:, :])

        return final_output




    def get_clusters_centers_tensor(self):
    # Return the tensor data of clusters_centers, not as a Parameter
        return self.clusters_centers.data