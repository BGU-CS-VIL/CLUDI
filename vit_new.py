import torch
import torch.nn as nn
import math
import einops
import torch.utils.checkpoint
import torch.nn.functional as F
import numpy as np
import math
from math import sqrt, ceil
from einops import rearrange, repeat, pack, unpack

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
    

def unpack_one(t, ps, pattern):
    return unpack(t, ps, pattern)[0]
import math
import warnings
def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a -
                       mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
    ATTENTION_MODE = 'flash'
else:
    try:
        import xformers
        import xformers.ops
        ATTENTION_MODE = 'xformers'
    except:
        ATTENTION_MODE = 'math'
print(f'attention mode is {ATTENTION_MODE}')


def timestep_embedding(timesteps, dim, max_period=10000):
    """
    Create sinusoidal timestep embeddings.

    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding


def patchify(imgs, patch_size):
    x = einops.rearrange(imgs, 'B C (h p1) (w p2) -> B (h w) (p1 p2 C)', p1=patch_size, p2=patch_size)
    return x


def unpatchify(x, channels=3):
    patch_size = int((x.shape[2] // channels) ** 0.5)
    h = w = int(x.shape[1] ** .5)
    assert h * w == x.shape[1] and patch_size ** 2 * channels == x.shape[2]
    x = einops.rearrange(x, 'B (h w) (p1 p2 C) -> B C (h p1) (w p2)', h=h, p1=patch_size, p2=patch_size)
    return x


class MPSiLU(nn.Module):
    def forward(self, x):
        return F.silu(x) / 0.596



class UViTLin(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=384, depth=4, num_heads=4, mlp_ratio=4.,
                 qkv_bias=False, qk_scale=None, norm_layer=nn.LayerNorm, mlp_time_embed=True, num_classes=-1,
                 use_checkpoint=False, conv=True, skip=True,num_clusters=10, embedding_dim =64,non_linear=False):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_classes = num_classes
        self.in_chans = in_chans
        self.self_condition = True
        self.embedding_dim = embedding_dim
        self.layer_norm =  nn.LayerNorm(self.embedding_dim, elementwise_affine=False, eps=1e-6)
        self.layer_norm2 =  nn.LayerNorm(self.embedding_dim, elementwise_affine=False, eps=1e-6)
        self.layer_norm3 =  nn.LayerNorm(self.embedding_dim, elementwise_affine=False, eps=1e-6)
        self.layer_norm4 =  nn.LayerNorm(self.embedding_dim, elementwise_affine=False, eps=1e-6)

        self.embedding_dim_start = (embedding_dim*2) if self.self_condition else embedding_dim

        self.feat_size = 384
        self.feat_size_start = 384
        self.embedding_dim_end = self.feat_size//2
        self.num_clusters = num_clusters
        self.dropout1 = nn.Dropout(0.15)
        self.dropout2 = nn.Dropout(0.15)
        self.dropout3 = nn.Dropout(0.15)
        self.channels = 1
        self.embed_dim = self.embedding_dim_end+self.feat_size
        self.time_embed_dim  =  self.embed_dim

        self.network_dim =  self.embed_dim #+ self.embedding_dim_end

        embed_dim = self.embed_dim
        self.time_embed = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            #nn.SiLU(),
            MPSiLU(),
            nn.Linear(4 * embed_dim, embed_dim),
        ) if mlp_time_embed else nn.Identity()
        self.last_layers = nn.ModuleList()



        self.network = nn.Sequential(
            nn.Linear(self.network_dim, self.network_dim),
            MPSiLU(),

            nn.Linear(self.network_dim, self.network_dim),
            MPSiLU(),
            nn.Linear(self.network_dim, self.network_dim),
            MPSiLU(),
            nn.Linear(self.network_dim, self.network_dim),
            MPSiLU(),
            nn.Linear(self.network_dim, self.embed_dim),
            MPSiLU(),
            nn.Linear(self.embed_dim, self.embedding_dim),
            MPSiLU(),

            nn.Linear(self.embedding_dim, self.embedding_dim),
        )

        self.norm = norm_layer(self.embedding_dim)
        self.norm2 = norm_layer(self.num_clusters)

        self.patch_dim = patch_size ** 2 * in_chans

        self.cluster_tokens = nn.Parameter(torch.randn(1, num_clusters*0+1, self.embed_dim))

        self.clusters_centers = nn.Parameter(torch.randn(1,num_clusters,self.embedding_dim)).requires_grad_(False)
        self.clusters_centers2 = nn.Parameter(torch.randn(1,num_clusters*2,self.embedding_dim)).requires_grad_(True)
        self.clusters_centers3 = nn.Parameter(torch.randn(1,num_clusters*4,self.embedding_dim)).requires_grad_(True)
        self.reducer = nn.Linear(self.embedding_dim*3, self.embedding_dim)
        self.input_data_layer = nn.Linear(num_clusters,  self.embed_dim)
        self.data_input_proj = nn.Sequential(
            nn.Linear(self.feat_size_start, self.feat_size_start, bias=True),
            #nn.SiLU(),
            MPSiLU(),

            nn.Linear(self.feat_size_start, self.feat_size_start, bias=True),
            nn.LayerNorm(self.feat_size_start, elementwise_affine=False, eps=1e-6),
            nn.Linear(self.feat_size_start, self.feat_size, bias=True),
        )

        self.cluster_input_proj = nn.Sequential(
            nn.Linear(self.embedding_dim_start, self.embedding_dim_start, bias=True),
            #nn.SiLU(),
            MPSiLU(),
            nn.Linear(self.embedding_dim_start,  self.embedding_dim_end, bias=True),
            nn.LayerNorm(self.embedding_dim_end, elementwise_affine=True, eps=1e-6),
            nn.Linear(self.embedding_dim_end,  self.embedding_dim_end, bias=True),
        )
        
        self.final_layer = nn.Linear(self.embedding_dim , self.embedding_dim)
        self.last_layer =  nn.Linear(self.embedding_dim, num_clusters)



        if non_linear:
            self.last_layer =  nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim),
                                             MPSiLU(),
                                             nn.Linear(self.embedding_dim,num_clusters),
                                             )

            self.last_layer2 =  nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim),
                                              MPSiLU(),
                                              nn.Linear(self.embedding_dim,num_clusters*2),
                                                                                           )


            self.last_layer3 =  nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim),
                                              MPSiLU(),
                                              nn.Linear(self.embedding_dim,num_clusters*4),
                                             )
        else:
            self.last_layer = nn.Sequential(  # nn.Linear(self.embedding_dim, self.embedding_dim),
                nn.Linear(self.embedding_dim, num_clusters),
            )

            self.last_layer2 = nn.Sequential(  # nn.Linear(self.embedding_dim, self.embedding_dim),
                nn.Linear(self.embedding_dim, num_clusters * 2),
            )

            self.last_layer3 = nn.Sequential(  # nn.Linear(self.embedding_dim, self.embedding_dim),
                nn.Linear(self.embedding_dim, num_clusters * 4),
            )
        self.weights = nn.Parameter(torch.ones(3))  # Weights for combining the embeddings


    def return_clusters_centers(self):
        return  F.normalize(self.clusters_centers,dim=-1)

    def return_embedding(self, x):
        # Normalize the cluster centers along the embedding dimension.
        clusters_centers = F.normalize(self.clusters_centers, dim=-1)  # Shape: [1, 1000, 128]

        weighted_sum = torch.matmul(x, clusters_centers.to(x.dtype))

        # Normalize the result and scale by sqrt(embedding_dim)
        return F.normalize(weighted_sum, dim=-1) * np.sqrt(self.embedding_dim)

    def return_embedding2(self,x):
        # Normalize the cluster centers along the embedding dimension.
        clusters_centers = F.normalize(self.clusters_centers2, dim=-1)  # Shape: [1, 1000, 128]

        weighted_sum = torch.matmul(x, clusters_centers.to(x.dtype))

        # Normalize the result and scale by sqrt(embedding_dim)
        return F.normalize(weighted_sum, dim=-1) * np.sqrt(self.embedding_dim)

    def return_embedding3(self,x):
        # Normalize the cluster centers along the embedding dimension.
        clusters_centers = F.normalize(self.clusters_centers3, dim=-1)  # Shape: [1, 1000, 128]
        weighted_sum = torch.matmul(x, clusters_centers.to(x.dtype))

        # Normalize the result and scale by sqrt(embedding_dim)
        return F.normalize(weighted_sum, dim=-1) * np.sqrt(self.embedding_dim)

    def weighted_emb(self,x1,x2,x3):
        l2_normalized_weights = self.weights / torch.norm(self.weights, p=2)
        softmax_weights = F.softmax(l2_normalized_weights / 0.1, dim=0)
        combined_emb = softmax_weights[0] * x1 + softmax_weights[1] * x2 + softmax_weights[2] * x3
        return combined_emb.to(x1.dtype)



    def return_last_layer(self,x):
        x = self.dropout3(x)
        return self.last_layer(x)


    def return_last_layer2(self,x):
        x = self.dropout3(x)
        return self.last_layer2(x)


    def return_last_layer3(self,x):
        x = self.dropout3(x)
        return self.last_layer3(x)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed'}

    def forward(self, cluster_assignments, data_features, timesteps,x_self_cond=False,return_feat=None,):

        cluster_assignments = cluster_assignments.to(data_features.dtype)
        if self.self_condition:
            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(cluster_assignments))
            cluster_assignments = torch.cat((x_self_cond, cluster_assignments), dim = -1)
        data_features = nn.functional.normalize(data_features, dim=-1, p=2)
        batch_size, num_points, _ = data_features.size()

        data_features = self.dropout1(self.data_input_proj(data_features))
        data_features = self.dropout1(data_features)
        cluster_assignments = self.dropout2(self.cluster_input_proj(cluster_assignments))
        #combined_features = self.cross_attention(data_features, cluster_assignments)
        combined_features = torch.cat((data_features,cluster_assignments),-1)
        #combined_features = data_features+cluster_assignments
        time_token = self.time_embed(timestep_embedding(timesteps, self.time_embed_dim))
        time_token = time_token.unsqueeze(dim=1)
        x = combined_features + time_token
        #x = torch.cat((time_token.repeat(1,x.shape[1],1), x), dim=-1)
        x = self.network(x)
        x = self.norm(x)
        final_output = self.final_layer(x)

        return final_output


